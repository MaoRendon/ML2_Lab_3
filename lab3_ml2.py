# -*- coding: utf-8 -*-
"""Lab3_ML2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mLxB3BnkW4YpdZFGWUrVX49SRrNJI5_R

# **WORKSHOP III**: Embebbeding Lab - Machine learning II

By:
- Sebastian Torres Sanchez
- Alvaro Gomez Penuela
- Hector Mauricio Rend√≥n

22/sept/2023 - University of Antioquia

## **Import package**

Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, and save you the time and resources required to train a model from scratch
"""

# import transformers
!pip install transformers

# Import numpy
import numpy as np

# Pipelines
from transformers import pipeline

# Models
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

# Metric
from sklearn.metrics.pairwise import cosine_similarity



"""## **Exercise 1**: Vector embeddings and its applications

**a) In your own words, describe what vector embeddings are and what they are useful for**

The vector embedding are a representation in a vector space of data such as words, sentences, paragraphs or documents in numerical forms, i.e., they are transformations of words to numerical values in space (vectors), trying to preserve the properties between words, such as semantics, in order to be able to be processed and perform analysis.

Vector embegginds can be used for recommendation systems, informatrion retrival, sentiment analysis, text classification, etc.

**See more details in attached PDF**.

**b) Example in Python**
"""

# Sample sentences
sentences = [
    'I love machine learning II',
    'Word embeddings are useful',
    'Natural language processing is fascinating',
    'Python is a popular programming language',
    'I will be sucsessful in data science'
]

# Tokenize the sentences into words
tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]

# Create Word2Vec model
Word2Vec_model = Word2Vec(tokenized_sentences, vector_size=50, window=5, min_count=1, sg=0)

# Get the vector for the word embeddings
vector_embedding_machine = Word2Vec_model.wv['machine']
vector_embedding_python = Word2Vec_model.wv['python']
vector_embedding_embeddings = Word2Vec_model.wv['embeddings']

# Show embedded vector for "machine"
print(f'Lenght vector embedding "machine": {len(vector_embedding_machine)}')
print(f'Vector embedding for "machine":\n {vector_embedding_machine}')
print('\nIt can be done with all the words')

"""## **Exercise 2**: Distance metrics in embeddings spaces

**a) What do you think is the best distance criterion to estimate how far two embeddings (vectors) are from each other? Why?**

There are many types of distances to calculate how far apart two embeddings are, because in the end these data are converted into vectors, natural or normal vector operations can be used for embedding, here are some of them:

* ***Euclidean Distance***:
	* Formula:
$$ \sqrt{\sum (x_{i}-y_{i})^{2})} $$
	* Use Case: It's straightforward and intuitive, but it can be sensitive to the scale of the features. It's commonly used when the features are continuous and have a clear geometric interpretation.
<br> <br/>
* ***Cosine Similarity***:
	* Formula: $$ \frac{x\cdot y}{\left \| x \right \| \left \| y \right \|} $$
	* Use Case: It measures the cosine of the angle between two vectors. It's particularly useful when the magnitude of the vectors doesn't matter, only the direction. This is common in text analysis and information retrieval.
<br> <br/>
* **Manhattan Distance** (L1 Norm):
	* Formula: $$ \sum \left | x_{i}-y_{i} \right | $$
	* Use Case: It's useful when the features are categorical or when you want to measure distance along axes in a grid-like structure.
<br> <br/>
* **Mahalanobis Distance**:
	* Formula: $$ \sqrt{(x-y)S^{T}(x-y)} $$ Where S^T is the covariance matrix of the data.
	* Use Case: It's useful when there is correlation between the features or when the data is not isotropically distributed.
<br> <br/>
* **Jaccard Distance**:
	* Formula: $$ 1-\frac{A\bigcap B}{A\bigcup B} $$
	* Use Case: Commonly used for comparing sets. It's particularly useful in tasks related to set similarity, such as document or text analysis.
<br> <br/>
* **Correlation Distance**:
	* Formula: $$ 1 - Corr(x, y) $$ Where Corr(x,y)is the Pearson correlation coefficient.
	* Use Case: It measures the correlation between two vectors and is particularly useful when you want to compare the shape of the vectors.

<br> <br/>
**See more details in attached PDF**.

**b) Example in Pyhton**

Let's use the previous embedded vectors for Machine, Python and Embeddings words to calculate the distances
"""

# Embedded words
print(f'Len of the embedded vector for Machine: {len(vector_embedding_machine)}')
print(f'Len of the embedded vector for Python: {len(vector_embedding_python)}')
print(f'Len of the embedded vector for Emdeddinds: {len(vector_embedding_embeddings)}')

# Measure Semantic Similarity
similarity_score = cosine_similarity([vector_embedding_machine], [vector_embedding_python])
print(f'Cosine similarity between "Machine" and "Python": {similarity_score[0][0]}')

# Measure Euclidean Similarity
euclidean_dist = np.linalg.norm(vector_embedding_machine - vector_embedding_python)
print(f'Euclidean distance between "Machine" and "Python": {euclidean_dist}')

# Measure Manhattan Similarity
manhattan_dist = np.sum(np.abs(vector_embedding_machine - vector_embedding_python))
print(f'Manhattan distance between "Machine" and "Python": {manhattan_dist}')

"""Note how each one gives different distance measurements, so their selection is an important factor in embedding applications.

## **Exercise 3**: Q&D System using embeddings

**Let us build a Q&A (question answering) system! For this, consider the following steps**

### **a) Choose a text**
Pick whatever text you like, in the order of 20+ paragraphs

For this item, chatgpt was used to recommend a text with more than 20 paragraphs and the text that was selected was the classic novel Pride and Prejudice" by Jane Austen.
"""

text = """

It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.

However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered as the rightful property of some one or other of their daughters.

"My dear Mr. Bennet," said his lady to him one day, "have you heard that Netherfield Park is let at last?"

Mr. Bennet replied that he had not.

"But it is," returned she; "for Mrs. Long has just been here, and she told me all about it."

Mr. Bennet made no answer.

"Do not you want to know who has taken it?" cried his wife impatiently.

"You want to tell me, and I have no objection to hearing it."

This was invitation enough.

"Why, my dear, you must know, Mrs. Long says that Netherfield is taken by a young man of large fortune from the north of England; that he came down on Monday in a chaise and four to see the place, and was so much delighted with it that he agreed with Mr. Morris immediately; that he is to take possession before Michaelmas, and some of his servants are to be in the house by the end of next week."

"What is his name?"

"Bingley."

"Is he married or single?"

"Oh! single, my dear, to be sure! A single man of large fortune; four or five thousand a year. What a fine thing for our girls!"

"How so? how can it affect them?"

"My dear Mr. Bennet," replied his wife, "how can you be so tiresome! You must know that I am thinking of his marrying one of them."

"Is that his design in settling here?"

"Design! nonsense, how can you talk so! But it is very likely that he may fall in love with one of them, and therefore you must visit him as soon as he comes."

"I see no occasion for that. You and the girls may go, or you may send them by themselves, which perhaps will be still better, for as you are as handsome as any of them, Mr. Bingley might like you the best of the party."

"My dear, you flatter me. I certainly have had my share of beauty, but I do not pretend to be anything extraordinary now. When a woman has five grown-up daughters, she ought to give over thinking of her own beauty."

"In such cases, a woman has not often much beauty to think of."

"But, my dear, you must indeed go and see Mr. Bingley when he comes into the neighbourhood."

"It is more than I engage for, I assure you."

"But consider your daughters. Only think what an establishment it would be for one of them. Sir William and Lady Lucas are determined to go, merely on that account, for in general, you know, they visit no newcomers. Indeed, you must go, for it will be impossible for us to visit him if you do not."

"You are over-scrupulous, surely. I dare say Mr. Bingley will be very glad to see you; and I will send a few lines by you to assure him of my hearty consent to his marrying whichever he chooses of the girls; though I must throw in a good word for my little Lizzy."

"I desire you will do no such thing. Lizzy is not a bit better than the others; and I am sure she is not half so handsome as Jane, nor half so good-humoured as Lydia. But you are always giving her the preference."

"They have none of them much to recommend them," replied he; "they are all silly and ignorant like other girls; but Lizzy has something more of quickness than her sisters."

"Mr. Bennet, how can you abuse your own children in such a way? You take delight in vexing me. You have no compassion for my poor nerves."

"You mistake me, my dear. I have a high respect for your nerves. They are my old friends. I have heard you mention them with consideration these last twenty years at least."

"Ah, you do not know what I suffer."

"But I hope you will get over it, and live to see many young men of four thousand a year come into the neighbourhood."

"It will be no use to us if twenty such should come, since you will not visit them."

"Depend upon it, my dear, that when there are twenty, I will visit them all."

Mr. Bennet was so odd a mixture of quick parts, sarcastic humour, reserve, and caprice, that the experience of three-and-twenty years had been insufficient to make his wife understand his character. Her mind was less difficult to develop. She was a woman of mean understanding, little information, and uncertain temper. When she was discontented, she fancied herself nervous. The business of her life was to get her daughters married; its solace was visiting and news.

To these highflown expressions Elizabeth listened with all the insensibility of distrust; and though the suddenness of their removal surprised her, she saw nothing in it really to lament; it was not to be supposed that their absence from Netherfield would prevent Mr. Bingley's being there; and as to the loss of their society, she was persuaded that Jane must cease to regard it, in the enjoyment of his.

"It is unlucky," said she, after a short pause, "that you should not be able to see your friends before they leave the country. But may we not hope that the period of future happiness to which Miss Bingley looks forward may arrive earlier than she is aware, and that the delightful intercourse you have known as friends will be renewed with yet greater satisfaction as sisters? Mr. Bingley will not be detained in London by them."

"Caroline decidedly says that none of the party will return into Hertfordshire this winter. I will read it to you."

"When my brother left us yesterday, he imagined that the business which took him to London might be concluded in three or four days; but as we are certain it cannot be so, and at the same time convinced that when Charles gets to town he will be in no hurry to leave it again, we have determined on following him thither, that he may not be obliged to spend his vacant hours in a comfortless hotel. Many of my acquaintances are already there for the winter; I wish that I could hear that you, my dearest friend, had any intention of making one of the crowd‚Äîbut of that I despair. I sincerely hope your Christmas in Hertfordshire may abound in the gaieties which that season generally brings, and that your beaux will be so numerous as to prevent your feeling the loss of the three of whom we shall deprive you."

"It is evident by this," added Jane, "that he comes back no more this winter."

"It is only evident that Miss Bingley does not mean that he should."

"Why will you think so? It must be his own doing. He is his own master. But you do not know all. I will read you the passage which particularly hurts me. I will have no reserves from you."

"Mr. Darcy is impatient to see his sister;

"""

"""### **b) Break the text down**
Split that text into meaningful chunks/pieces.

The text was divided by paragraphs, ChatGPT delivered the text organized by paragraphs, where each line is a paragraph of the original text, therefore, it was decided to divide this text into these paragraphs.
"""

# Split the text into a list of paragraphs
paragraphs = text.split("\n\n")

# Remove empty text
paragraphs.pop(0)
paragraphs.pop(-1)

print(f'The numbers of chunks are {len(paragraphs)}')

print('Short view of the total paragraphs:\n')
paragraphs[:3]

"""### **c) Emdegging**
Implement the embedding generation logic. Which tools and approaches would help you generate them easily and high-level?

To implement the vector embedding you can use many methods that can be found in the literature and whose implementations already exist in Python

**See more details in attached PDF**
<br> <br/>

It is also possible to use already trained models, ready to use and whose accuracy is validated. This type of already trained models can be found in the page *'Hugging face'* in a very simple way, for this we will implement a logic of embedding generation using the library *'transformer'* with which we can use the models developed and shared by the page 'Hugging face', in this case we will use the category *'question-answering'* and we will use the model *'distilbert-base-cased-distilled-squad'* which is one of the best valued of the page.

**Example of Pipeline**

Example of how to use this library, let's create a question-answering model based on DistilBERT (a lighter version of BERT) pretrained on the Stanford Question Answering Dataset (SQuAD).
"""

# Defining the Question-Answering Model
question_answerer = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')

# Defining the context
context = r"""
Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a
question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune
a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.
"""

# Asking the model a question:
result = question_answerer(question="What is a good example of a question answering dataset?",  context=context)

# Printing the answer and score
print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}")
print(f"start position in text: {result['start']}, end position in text: {result['end']}")

"""### **d) Train and ask**
For every question asked by the user, return a sorted list of the N chunks/pieces in your text that relate the most to the question. Do results make sense?
"""

# Input the question
question = input('Input the question: ')

''' Is he married or single?'''

# Input number of chunks
N_chunks = int(input('Input numbers of Chunks: '))

''' 5 '''

# Defining the Question-Answering Model
question_answerer = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')

# Store the results
results = []
for paragraph in paragraphs:
  result = question_answerer(question=question,  context=paragraph)

  result['chunk'] = paragraph

  results.append(result)

# Sort the results
sorted_result = sorted(results, key=lambda x: x['score'], reverse = True)

# Showing the chunks
for i in range(N_chunks):
  chunk_result = sorted_result[i]['chunk']
  print(f'chunk {i}: {chunk_result}')

# Showing the result
for i in range(N_chunks):
  chunk_result_answer = sorted_result[i]['answer']
  print(f'answer chunk {i}: {chunk_result_answer}')

"""Generating a model with all the text without separating in chunk/pieces

"""

result = question_answerer(question=question,  context=text)

print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}")
print(f"start in text: {result['start']}, end in text: {result['end']}")

"""The results make sense because the chunk/pieces that appear when the question is generated or chunk/pieces in which the words that were used in the question appear, due to the fact that the separation was made by paragraph and not according to a semantic order, the answers are not adequate at a semantic level, but at a grammatical level.

## **Exercise 4**: How improve the Q&A performance?

**What do you think that could make these types of systems more robust in terms of semantics and functionality?**

Improving the robustness of vector word embedding systems, such as Word2Vec, GloVe, or more advanced models like BERT and GPT, is a complex and ongoing research challenge.

Some strategies and considerations that could enhance the robustness of such systems in terms of semantics and functionality:

* **Diverse Training Data**: Use a diverse and comprehensive corpus for training word embeddings. Ensure that the data includes a wide range of domains, languages, and writing styles to capture a broader spectrum of semantics and concepts.
<br> <br/>
* **Multilingual Training**: Train embeddings on multilingual corpora to enable cross-lingual transfer of semantic information. This can help improve the understanding of words and phrases in multiple languages.
<br> <br/>
* **Fine-Tuning**: Fine-tune pre-trained embeddings on domain-specific data or tasks to adapt them to specific use cases. Fine-tuning allows you to capture domain-specific semantics.
<br> <br/>
* **Dynamic Word Embeddings**: Explore techniques that allow embeddings to change dynamically based on context or time, reflecting evolving language semantics. This can be crucial for handling semantic drift.
<br> <br/>
* **Evaluation Metrics**: Develop and use more sophisticated evaluation metrics that assess not only syntactic but also semantic properties of word embeddings. Metrics like word similarity, analogy, and downstream task performance can provide a better assessment of semantic quality.
<br> <br/>
* **Bias Mitigation**: Address and mitigate biases present in word embeddings, such as gender or racial biases, to ensure that embeddings are more fair and representative of the real world.
<br> <br/>
* **Conceptual Understanding**: Incorporate external knowledge sources, such as knowledge graphs or ontologies, to enhance the embeddings' understanding of concepts and relationships between words.
<br> <br/>
* **Adversarial Testing**: Subject word embeddings to adversarial testing to identify vulnerabilities and improve their robustness against attacks or perturbations.
<br> <br/>
* **Regularization Techniques**: Apply regularization techniques to reduce overfitting during training and improve the generalization of word embeddings.
<br> <br/>
* **Human-in-the-Loop**: Involve human experts in the development and validation process to ensure that the embeddings capture the intended semantics and are free from biases.
<br> <br/>
* **User Feedback**: Continuously gather user feedback to identify and rectify any issues or limitations in the embeddings' semantics and functionality.


**See more details in attached PDF**.
"""